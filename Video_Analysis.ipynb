{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb54fda-b74f-4da8-afea-0b3afb8f086a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to video Analysis):\n",
    "\n",
    "This Python script utilizes Google Gemini, a multimodal LLM (Large Language Model), to analyze a video and identify scene changes, specifically focusing on detecting potential points for ad insertion. The script prompts Gemini to select the 10 most suitable scene transitions across the video, distributing them throughout the beginning, middle, and end. These points are chosen to minimize interruptions for viewers and ensure optimal ad placement.\n",
    "\n",
    "The Gemini model provides metadata for each identified scene change, including the timestamp, reason for the scene change, summary, transition type, narrative role, dialogue intensity, and character types. This metadata is returned in a consistent JSON structure, thanks to Gemini's controlled output feature, making it suitable for downstream processing without additional transformations. The script converts this structured JSON response into a Pandas DataFrame for storage in a database like BigQuery for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df6686ab-3e09-4559-9bf2-f348fd9fe56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import vertexai  # Import the Vertex AI library for initializing and using the Gemini model\n",
    "from google.cloud import storage  # Google Cloud Storage client for handling GCS\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,        # Configuration settings for Gemini's response generation\n",
    "    GenerativeModel,         # Class representing the generative model used for generating responses\n",
    "    Part,                    # Used to define parts of multimodal content like videos\n",
    "    Content,                 # Represents the content used in the generation request\n",
    "    GenerationResponse,      # Structure to handle the response from Gemini\n",
    ")\n",
    "\n",
    "# Import the ImageGenerationModel from the Vertex AI preview vision models package.\n",
    "# This model is used for generating images based on user prompts, leveraging GCP's Vertex AI services.\n",
    "from vertexai.preview.vision_models import ImageGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0adc2d-8ecf-4aff-9612-d49ae7af0e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the GCP project and location where Vertex AI is being used\n",
    "project_id = \"qwiklabs-gcp-02-32addd023448\"\n",
    "location = \"us-central1\"\n",
    "\n",
    "# Initialize Vertex AI with the project ID and location to use the Gemini model\n",
    "vertexai.init(project=project_id, location=location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604301a2-4f95-4fe8-879f-9b58d79011e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt that will be sent to Gemini.\n",
    "# This prompt explains the task to analyze the video and identify the best scene changes for ad placement.\n",
    "prompt = '''\n",
    "       I have a video that I need you to analyze for ad placement by detecting scene changes, \n",
    "       also known as shot boundaries. I need to identify the 10 best scene changes across the \n",
    "       entire movie, which are the best potential points for ad placement as they minimize \n",
    "       interruptions for viewers. These scene changes should be selected from all parts of the movie: \n",
    "       the beginning, middle, and the very end. Make sure you distribute the selected scenes evenly across \n",
    "       the entire movie.\n",
    "       For each of these scene changes, please provide:\n",
    "\n",
    "        timestamp: The exact timestamps indicating where the scene change occurs. Make sure that the timestamp of scenes are matched those in the original movie,\n",
    "        reflecting its position accurately. The timestamps must exactly match those in the original movie.\n",
    "        \n",
    "        reason: The reason why this is a scene change and why it is a good location for ad placement. the reason \n",
    "        should be very specific. Summarize the story after and before the scene and explain why \n",
    "        between these two scenes is a good place for an ad.\n",
    "        \n",
    "        summary: A brief summary of the scene before the change.\n",
    "        \n",
    "        transition_feeling: The main feeling that the transition makes in viewers like excitement, peace, fear, etc.\n",
    "        \n",
    "        transition_type: The method used to switch from one scene to another like cuts, fades, dissolves, etc.\n",
    "        \n",
    "        narrative_type: The main role or significance of the scene in the storyline like pivotal, climatic, conflict, etc.\n",
    "        \n",
    "        dialogue_intensity: The amount and intensity of dialogue in the scene like monologue, dialogue, narration, debate, etc.\n",
    "\n",
    "        characters_type: The types of the most important character involved in the scene transition like protagonist, antagonist, supporting, etc.\n",
    "        \n",
    "        scene_categories:  Classification of the scene before the change into the categories such as action, drama, comedy, etc.\n",
    "      '''      \n",
    "\n",
    "# Define the expected response schema to ensure the output JSON is structured correctly\n",
    "response_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"timestamp\": {\"type\": \"string\"},\n",
    "            \"reason\": {\"type\": \"string\"},\n",
    "            \"transition_feeling\": {\"type\": \"string\"},\n",
    "            \"transition_type\": {\"type\": \"string\"},\n",
    "            \"narrative_type\": {\"type\": \"string\"},\n",
    "            \"dialogue_intensity\": {\"type\": \"string\"},\n",
    "            \"characters_type\": {\"type\": \"string\"},\n",
    "            \"scene_categories\": {\"type\": \"string\"},\n",
    "        },\n",
    "        # Ensure that these properties are always present in the output\n",
    "        \"required\": [\"timestamp\", \"reason\",\"transition_feeling\",\"transition_type\",\"narrative_type\",\n",
    "                    \"characters_type\",\"scene_categories\"],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda69fbe-4f73-4125-a3f4-209f3468dcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the version of the Gemini model to be used\n",
    "model_id = \"gemini-1.5-pro-001\"  \n",
    "model = GenerativeModel(model_id)\n",
    "\n",
    "# Set up the generation configuration to control Gemini's response\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0,  # Set the temperature to 0 for consistent output\n",
    "    response_mime_type=\"application/json\",  # Expect the response to be in JSON format\n",
    "    response_schema=response_schema  # Use the predefined response schema for structured output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a6a0b6-7b7a-4638-9e79-06cc048308d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the URL of the video file stored in Google Cloud Storage\n",
    "video_file_url = 'gs://video_demo_test/wakeup_princess.mp4'\n",
    "\n",
    "# Load the video file from Google Cloud Storage as an input to Gemini\n",
    "video_file = Part.from_uri(video_file_url, mime_type=\"video/mp4\")\n",
    "\n",
    "# Combine the video file and the prompt into a single input to the Gemini model\n",
    "contents = [video_file, prompt]\n",
    "\n",
    "# Generate content from Gemini by passing the contents and the generation configuration\n",
    "response = model.generate_content(contents, generation_config=generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3ec957-03b6-42f1-abcd-b04befc18494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the JSON response from Gemini into a Python dictionary\n",
    "json_response = json.loads(response.text)\n",
    "\n",
    "# Convert the JSON response into a Pandas DataFrame for easier analysis and storage\n",
    "df_response = pd.DataFrame(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da9b492a-b0b6-4b32-bfe3-51af1231f9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_response.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01aad1-92b1-4f4c-9938-2211bb3c7bc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to Image Generation\n",
    "\n",
    "This script leverages GCP's image generation model to create images based on a user-defined prompt. The model allows for customizing the image output through two primary inputs:\n",
    "1. **Image Prompt**: A description of the desired scene or elements to be generated.\n",
    "2. **Negative Prompt**: Specifies objects or elements to avoid in the generated images.\n",
    "\n",
    "The script can generate up to 4 images at a time, with the number of images controlled by a parameter. Users can choose between multiple pre-trained models, such as:\n",
    "- **imagen-3.0-generate-001**: The default model for image generation.\n",
    "- **imagen-3.0-fast-generate-001**: A low-latency version for faster image generation.\n",
    "- **imagegeneration@002**: An earlier version of the image generation model.\n",
    "\n",
    "Once images are generated, they can be accessed, displayed, and saved to disk. The script includes parameters to control the **language** and **aspect ratio** of the generated images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be65d8-dbc7-4953-8f6b-313323ea3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image generation model from GCP's pre-trained model 'imagen-3.0-generate-001'.\n",
    "image_model = ImageGenerationModel.from_pretrained(\"imagen-3.0-generate-001\")\n",
    "\n",
    "# Other available image generation models:\n",
    "# 1. imagen-3.0-fast-generate-001: Low-latency image generation model for faster results.\n",
    "# 2. imagegeneration@002: Older model (Imagen 2.00), available for use if needed.\n",
    "\n",
    "# Define the image prompt describing the desired scene.\n",
    "image_prompt = '''\n",
    "        Having banana cake with a hot tea while watching rainy weather from a rustic window in Paris with the view of Eiffel\n",
    "'''\n",
    "\n",
    "# Define the negative prompt to exclude specific elements from the generated images.\n",
    "# Here, we instruct the model to avoid 'tea pot' and 'banana' in the output images.\n",
    "negative_prompt = 'tea pot, banana',\n",
    "\n",
    "# Generate up to 4 images using the model, based on the provided image and negative prompts.\n",
    "images = image_model.generate_images(\n",
    "        prompt=image_prompt,        # The prompt describing the desired image.\n",
    "        negative_prompt='tea pot, banana',  # Elements to exclude from the generated images.\n",
    "        number_of_images=4,         # Specify how many images to generate (up to 4).\n",
    "        language=\"en\",              # Language of the prompt (English in this case).\n",
    "        aspect_ratio=\"1:1\",         # Aspect ratio of the generated images (square format).\n",
    "    )\n",
    "\n",
    "# 'images' is an iterable object containing the generated images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b04ccaf-2323-4088-81b1-dc3bcff8bc90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the first generated image.\n",
    "images[0].show()\n",
    "\n",
    "# Save the first generated image to disk.\n",
    "images[0].save()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
